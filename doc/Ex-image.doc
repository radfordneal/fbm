

EXAMPLES OF BAYESIAN NEURAL NETWORK IMAGE MODELS

One prominent area of application for neural network models is
classification of images.  Here, a fairly simple artificial example of
such an image classification problem is presented.  Several Bayesian
neural network models are applied to this task, including ones that
use convolutional connections.  Fitting such models by simple gradient
descent is also presented.

These examples also illustrate how the training data can be divided in
several ways into a set of cases used for fitting, and a set of
validation cases, which can be used to assess performance.  Although
Bayesian methods do not require a validation set in theory, in
practice, some check on whether good performance has been achieved is
advisable before actually using predictions from a Bayesian model, to
guard against statistical mistakes, poor convergence of MCMC methods,
or, if nothing else, out-and-out bugs in the scripts or programs used.

The command files for this example are in the ex-image sub-directory.


The image classification problem used in these examples.

The program in igen.c generates artificial 6x6 monochrome images in
which one of the symbols "+", "X", "O", or "H" appears in some 3x3
patch in the image.  The classification task is to predict from the 36
pixels of an image which symbol appears in some patch in the image
(whose location could be any of the sixteen possibilities).  The file
'idata' has 4000 generated cases, the first 1600 of which are used for
training, and the remaining 2400 for testing.

Generation of an image starts by randomly generating background pixel
values from a Gaussian distribution with mean zero and standard
deviation one, independently for each of the 36 pixels.  A location
for a symbol is then randomly gnerated from the 16 possible positions.
(Note that the centre position of the symbol cannot be in the first or
last row, or the first or last column, leaving four possible row
positions and four possible column positions.)  The identity of the
symbol is also picked randomly from "+", "X", "O", and "H" (coded as
0, 1, 2, or 3), with each symbol having probability 1/4.  The pixels
in the 3x3 patch where the symbol is located are then replaced by the
following patterns for each symbol:

       "+"         "X"         "O"         "H"

    -1 +1 -1    +1 -1 +1    +1 +1 +1    +1 -1 +1
    +1 +1 +1    -1 +1 -1    +1 -1 +1    +1 +1 +1
    -1 +1 -1    +1 -1 +1    +1 +1 +1    +1 -1 +1

Finally, Gaussian noise with mean zero and standard deviation 0.5 is
added to all pixels, and the pixel values are rounded to three decimal
places.

When passed an argument, the igen program prints each image, with the
true location and identity of the symbol, as well as the probabilities
for each symbol that can be inferred from the pixels using the true
model of how these images are generated.  Here are three of the images
generated:
  

  Case 2, Class +(0), Centred at 1,3, PP: +0.999 X0.000 O0.001 H0.000
  
        0      1      2      3      4      5       0 1 2 3 4 5
  0   -0.626 +1.056 -0.845 +0.762 -0.477 -0.837    O # O #   O
  1   +0.723 +0.971 +1.134 +1.040 +1.059 +0.147    # # # # #
  2   -1.095 -2.091 -0.671 +0.971 -0.649 +0.715    O O O # O #
  3   +0.787 -1.511 +1.770 +1.620 +0.221 -0.582    # O # #   O
  4   +0.535 -1.675 -0.308 -0.815 -0.786 +0.273    # O   O O
  5   -0.215 +0.335 -1.835 -0.286 +1.338 +0.336        O   #
    
    
  Case 11, Class X(1), Centred at 2,4, PP: +0.157 X0.843 O0.000 H0.000
  
        0      1      2      3      4      5       0 1 2 3 4 5
  0   -0.818 +0.064 -0.260 +0.159 +0.272 +0.009    O
  1   +0.236 +1.215 -0.756 -0.263 -1.089 +1.030      # O   O #
  2   -1.443 -1.099 +0.870 -0.965 +1.381 -1.830    O O # O # O
  3   -1.044 +1.340 +0.445 +1.512 -1.571 +0.600    O #   # O #
  4   -0.776 +0.444 +1.656 -1.832 +0.346 -0.823    O   # O   O
  5   -0.431 +0.744 +1.261 +3.750 -1.659 +1.955      # # # O #
  
  
  Case 60, Class O(2), Centred at 4,1, PP: +0.000 X0.000 O0.418 H0.582
  
        0      1      2      3      4      5       0 1 2 3 4 5
  0   +1.576 -1.255 +1.379 +1.325 -0.615 +0.105    # O # # O
  1   +0.734 -0.416 -1.230 -0.001 -1.100 +0.948    #   O   O #
  2   +0.837 -1.332 +0.800 -2.698 +1.139 +0.417    # O # O #
  3   +1.236 +0.820 +1.372 -0.339 +0.924 +0.754    # # #   # #
  4   +0.508 -1.105 +0.611 +2.562 -1.724 -1.193    # O # # O O
  5   +0.715 +1.017 +0.755 -0.284 -1.175 +1.185    # # #   O #


The diagrams at the right show the pixels of the image thresholded at
values less than -0.5 (O), greater than +0.5 (#), and between -0.5 and
+0.5 (space).  For the second image, noise in the pixels has made the
symbol less certain (probability 0.843 for the correct symbol) than
for the first case (probability 0.999 for the correct symbol), and for
the third image, the correct symbol has lower probability (0.418) than
one of the incorrect symbols (0.582) - with all these probabilities
computed assuming the true generation model is known.

The igen program computes the performance on test cases when using the
true model, which is an upper limit on what can be accomplished when
learning a model from the training cases (unless one is just lucky).
Here is the summary:

  Error rate on test cases with true model: 0.082
  Average squared error for test cases with true model: 0.117
  Average log probability for test cases with true model: -0.205


A fully-connected Bayesian neural network model.

We can first try a neural network model that has no built-in knowledge
of how the 36 pixel values are arranged in a 6x6 image, or any
specific knowledge that the symbol to be detected is in a 3x3 patch of
this image.  The model simply feeds the 36 inputs into a hidden layer
with 80 tanh units, which in turn feeds into an output layer with 4
"softmax" units, which are used to define the class probabilities, as
in the classification example in Ex-netgp-c.doc.

This model can be specified with the following commands:

  net-spec $log 36 80 4 \
            / ih=0.05:3::1.5 bh=0.3:2 ho=0.1:3::4 bo=0.1:2
  model-spec $log class

Here, $log represents the name of the log file.  This is followed by
the specification that there are 36 input, 80 hidden units, and 4
units used to produce class probabilities, using the "softmax" scheme.

The prior specification for input-to-hidden weights, of 0.05:3::1.5,
does not allow for different inputs to have different relevance (hence
no number between the secnd and third colons, so there is no variation
between input groups), since we will assume that it is known that the
inputs are almost equally relevant (since the symbol to be recognized
might occur anywhere in the image).  The final 1.5 in the prior
specification gives the input-to-hidden weights a heavy-tailed prior
(a t distribution with 1.5 degrees-of-freedom), allowing for the
possibility that the input-to-hidden connections are effectively
sparse, with a hidden unit looking mostly at only a few of the pixel
values.  The hidden-to-outuput weights also have a (less extreme)
heavy-tailed prior, allowing for some of the hidden units to have
small weights to some of the output units.

A scheme allowing cross-validation assessments of performance is used,
in which four training runs are done, each on 3/4 of the training
data, with the omitted 1/4 allowing for monitoring of performance (to
see, for example, whether it is advisable to do further MCMC
iterations).  Of course, making such assessments based on the test
data would be "cheating" (since in real life, the test targets, and
probably the test inputs as well, are not available when training).

The data specification used is therefore as follows:

  data-spec $log 36 1 4 / idata-train@-$start:$end . idata-train@$start:$end .

Here $start and $end represent the range of training cases comprising
the validation set (here specified as the "test" set), with the
remainder of the 1600 training cases used for fitting the model.  In
the four runs that are done, $start and $end will be set to 1 and 400,
401 and 800, 801 and 1200, and 1201 and 1600.  (These runs will use
log files of ilog1.net1, ilog1.net2, ilog1.net3, and ilog1.net4.)

When final predictions for test cases are made, the "test" data
specified in this data-spec command will be overridden by the actual
test data, which is in idata-test.

The numbers in the data-spec command specify that there are 36 inputs
and 1 target, which has 4 possible values (coded as 0, 1, 2, 3).

With the network model and data specified, training is done as follows:

  rand-seed $log $run

  net-gen $log fix 0.1

  mc-spec $log repeat 120 heatbath hybrid 200:20 0.1
  net-mc $log 1

  mc-spec $log repeat 60 sample-sigmas heatbath hybrid 400:40 0.15
  net-mc $log 2

  mc-spec $log repeat 24 sample-sigmas heatbath 0.9 hybrid 1000:100 0.21 negate
  net-mc $log 3000

Here, $run is 1, 2, 3, or 4, identifying which of the four runs this
is, which affects the random number seed used.  The hyperparameters
are initialized to the fairly small value of 0.1, and 120 HMC updates
are done with the hyperparameters fixed at this value, so that the
parameters will take on reasonable values for the data before trying
to update the hyperparameters.  After this, 60 HMC updates are done
with hyperparmeters updated before each HMC update, using a moderate
number (400) of leapfrog steps.  This is intended to get the
hyperparameters to reasonable values (given how well the data has been
fitted so far).  The main sampling iterations are then done, in which
updates for the hyperparameters alternate with HMC iterations using a
longer trajectory of 1000 leapfrog iterations.  Each of the 2998 saved
states from this main sampling phase are the result of 24 such pairs
of hyperparameter updates and HMC updates for parameters.

The HMC updates all use the "windowed" method, to increase acceptance
rate, with the windows being 1/10 of the trajectory.  Relatively small
leapfrog stepsizes are used initially, to guard against rejection
rates being high when starting at an atypical location.  The stepsize
of 0.21 for the main sampling phase produces a rejection rate of 1% to
3%.  To further reduce random walk behaviour, without reducing the
number of hyperparameter updates, the "persistent momentum" version of
HMC is used for the final sampling phase.

Each full iteration of this sampling procedure takes about 40 seconds
on the system used (Ex-test-system.doc), for a total of about 2000
minutes (about 33 hours) for the 3000 iterations done.  The command
script in ex-image/icmds1.net does all four runs in parallel, so this
will be the total wall-clock time if the system used has at least four
processor cores.

Various plots can be made to check whether these runs are sampling (or
have sampled) well, or whether more iterations (or a better scheme)
are needed.  The performance of the HMC updates can be checked by
plotting the point on the trajectory moved to in each iteration:

  net-plt t m ilog1.net? | plot-points

As seen in ilog1-m.png (with different colours for each run), most of
the moves (but not all) are to the accept window of the trajectory, so
the stepsize used is reasonable.

The fit (squared error) for the training data can be checked with

  net-plt t b ilog1.net? | plot-points

As seen in ilog1-b.png, all four runs seem to reach the equilibrium
distribution for this value in 100 iterations or less.  (The
equilibrium distributions differ slightly between runs because they
have different training and validation sets.)

The squared error for each iteration on the validation cases can be
seen with

  net-plt t B ilog1.net? | plot-points

As seen in ilog1-BB.png, this quantity show higher long-term
autocorrelation than the training fit, but again may have reached
equilibrium in less than 100 iterations.  Note that although this
quantity is useful in seeing how well the Markov chain is sampling,
the squared error on validation cases from a single iteration does NOT
tell one how good the predictions on these validation cases found by
averaging many iterations would be - poor predictions from a single
iteration may be the result of the model producing high confidence
predictions at each iteration, that vary from iteration to iteration,
so that the averaged prediction has much lower confidence (and perhaps
much better performance).  So there is not necessarily anything wrong
when the error on validation cases from individual networks is high.

We can look at the main hyperparameters (standard deviations for
input-hidden weights, hidden biases, and hidden-output weights) at
each iteration of the first run as follows:

  net-plt t h1h2h3 ilog1.net1 | plot-points-log-scale

This data is best plotted on a log scale, because the three
hyperparameters differ greatly in magnitude, and also vary greatly
over iterations.  The plot is in ilog1-h1h2h3.png.
  
Once all runs have finished, we can assess performance on the actual
test set as follows (with the first 1000 iterations discarded as
"burn-in", which seems sufficient given the plots above):

  net-pred mnpa ilog1.net1 1001: ilog1.net2 1001: \
                ilog1.net3 1001: ilog1.net4 1001: / idata-test .

The output is as follows:

  Number of iterations used: 8000

  Number of test cases: 2400

  Average log probability of targets:    -1.030+-0.012
  Fraction of guesses that were wrong:    0.4517+-0.0102
  Average squared error guessing mean:    0.57219+-0.00671

Only slightly worse results would be obtained using runs only one
tenth as long (3.3 hours).  The command

  net-pred mnpa ilog1.net1 101:300 ilog1.net2 101:300 \
                ilog1.net3 101:300 ilog1.net4 101:300 / idata-test .

produces the following output:

  Number of iterations used: 800

  Number of test cases: 2400

  Average log probability of targets:    -1.032+-0.012
  Fraction of guesses that were wrong:    0.4579+-0.0102
  Average squared error guessing mean:    0.57331+-0.00672

Note that guessing by ignoring the inputs and assigning equal
probabilities to the four classes gives an average log probability of
-1.386, an error rate of 0.75, and an average squared error of 0.75.
So this model performs significantly better than random guessing.
However, the error rate of 45% is considerably more than the 8% error
rate that is achievable with knowledge of the true generating process,
so there is much room for improvement.


A convolutional Bayesian neural network model.

Supposing that we know that the symbol to be recognized is 3 pixels by
3 pixels, and that it may appear at any position in the image, it
makes sense to use a convolutional network, that uses several filters
looking at 3x3 patches of the image.  Each such filter is defined by
the 9 weights on connectiions from a 3x3 patch at some position in the
image to a hidden unit, with the weights for all possible positions
being the same.  We might expect that four filters, one for each
class, would be sufficient, but the network defined here will allow
for five filters, which may make it easier to find the four needed,
since a "spare" filter may allow locally-optimal modes to be more
easily escaped.

The simplest model of this type has a single hidden layer with 80
hidden units (5 filters, each needing 16 units for all possible symbol
positions), which connect to the four output units used to define
class probabilities.  Such a network model can be specified as
follows:

  net-spec $log 36 80 input-config:iconfig 4 \
                / ih=0.2:2 bh=0.3:2 ho=0.1:3::4 bo=0.1:2
  model-spec $log class

The "input-config" flag after the specification of the size of the
hidden layer says that the configuration of the weights for
connections from inputs to hidden units is specified in the file
"iconfig", whose contents are as follows:

  # Configuration for convolutional layer connections for model of test images.
  
  D=6  # image width and height
  P=3  # patch width and height
  F=5  # number of filters
  
  [ 1 1 1 ]
  
  D-P+1{  # Loop over vertical positions of patch
  
    D-P+1{  # Loop over horizontal positions of patch
  
      P{  # Loop over rows of patch
  
        P{  # Loop over pixels within row of patch
  
            = = =   F-1{ = + + }  # Connections for all filters 
  
            [ + = +F ]
        }  
  
        [ +D = = ]   P[ = = +F ]
      }
  
      [ + +F 1 ]
    }
  
    [ +D = 1 ]   D-P+1[ = +F 1 ]
  }

See net-config.doc for an explanation of the specification language
used above.  The effect of this specification is to set up the
convolutional connections, with 45 weights defining 5 filters (each
looking at 9 pixels), which are shared by 16x9x5=720 connections
(which is less than the 36x80=2880 possible connections).

The other commands used are almost the same as for the fully-connected
network model above.  The only difference is that the stepsize for the
final sampling phase is reduced from 0.21 to 0.2 to counter the fact
that at one stage (iterations 500 to 1000) the rejection rate with
this model goes up a bit.

Each full iteration takes about 37 seconds on the system used
(Ex-test-system.doc), for a total of 31 hours (with the four runs done
in parallel).  This is a bit faster then for the fully-connected
network, since there are fewer input-to-hidden connections (an
advantage partially offset by the overhead of processing the
non-contiguous configuration of connections).

As above, we can plot the 'm' (move point), 'b' (squared error on
training data), and 'B' (squared error on validation data) quantities
for the four runs using net-plt.  The results can be seen as
ilog2-m.png, ilog2-b.png, and ilog2-BB.png.  The main hyperparameter
values for the first run can also be plotted, as above, giving the
result in ilog2-h1h2h3.png.

Once all runs have finished, we can assess performance on the actual
test set as follows (with the first 1000 iterations discarded as
"burn-in", which seems more than sufficient given the plots above):

  net-pred mnpa ilog2.net1 1001: ilog2.net2 1001: \
                ilog2.net3 1001: ilog2.net4 1001: / idata-test .

The output is as follows:

  Number of iterations used: 8000

  Number of test cases: 2400

  Average log probability of targets:    -0.783+-0.019
  Fraction of guesses that were wrong:    0.2812+-0.0092
  Average squared error guessing mean:    0.41774+-0.00981

Only slightly worse results would be obtained using runs only one
tenth as long (3.1 hours).  The command

  net-pred mnpa ilog2.net1 101:300 ilog2.net2 101:300 \
                ilog2.net3 101:300 ilog2.net4 101:300 / idata-test .

produces the following output:

  Number of iterations used: 800

  Number of test cases: 2400

  Average log probability of targets:    -0.779+-0.018
  Fraction of guesses that were wrong:    0.2879+-0.0092
  Average squared error guessing mean:    0.41990+-0.00960

These results are considerably better than were obtained above when
using a fully-connected network, demonstrating the advantage of using
a convolutional network when, as here, it is known that it fits the
structure of the problem.


Adding another hidden layer.


Fitting the models with gradient descent.
