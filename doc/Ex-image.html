<html>
<body>
<pre>


EXAMPLES OF BAYESIAN NEURAL NETWORK IMAGE MODELS

One prominent application of neural network models is for the
classification of images.  Here, a fairly simple artificial example of
such an image classification problem is presented.  Several Bayesian
neural network models for this task are presented, including ones that
use convolutional connections.  Fitting such a model by simple
gradient descent is also presented.

These examples also illustrate how the training data can be divided in
several ways into a set of cases used for fitting, and a set of
validation cases, which can be used to assess performance.  Although
Bayesian methods do not require a validation set in theory, in
practice, some check on whether good performance has been achieved is
advisable before actually using predictions from a Bayesian model, to
guard against statistical mistakes, poor convergence of MCMC methods,
or, if nothing else, out-and-out bugs in the scripts or programs used.

The command files for this example are in the ex-image sub-directory.


The image classification problem used in these examples.

The program in igen.c generates artificial 6x6 monochrome images in
which one of the symbols "+", "X", "O", or "H" appears in some 3x3
patch in the image.  The classification task is to predict from the 36
pixels of an image which symbol appears in some patch in the image
(whose location could be any of the sixteen possibilities).

Generation of an image starts by randomly generating backgroup pixel
values from a Gaussian distribution with mean zero and standard
deviation one, independently for each of the 36 pixels.  A location
for a symbol is then randomly gnerated from the 16 possible positions.
(Note that the centre position of the symbol cannot be in the first or
last row, or the first or last column, leaving four possible row
positions and four possible column positions.)  The identity of the
symbol is also picked randomly from "+", "X", "O", and "H" (coded as
0, 1, 2, or 3), with each symbol having probability 1/4.  The pixels
in the 3x3 patch where the symbol is located are then replaced by
the following patterns for each symbol:

       "+"         "X"         "O"         "H"

    -1 +1 -1    +1 -1 +1    +1 +1 +1    +1 -1 +1
    +1 +1 +1    -1 +1 -1    +1 -1 +1    +1 +1 +1
    -1 +1 -1    +1 -1 +1    +1 +1 +1    +1 -1 +1

Finally, Gaussian noise with mean zero and standard deviation 0.6 is
added to all pixels, and the pixel values are rounded to three decimal
places.

When passed an argument, the igen program prints each image, with the
true location and identity of the symbol, as well as the probabilities
for each symbol that can be inferred from the pixels using the true
model of how these images are generated.  Here are three of the images
generated:
  

  Case 2, Class +(0), Centred at 1,3, PP: +0.992 X0.000 O0.007 H0.001
  
        0      1      2      3      4      5       0 1 2 3 4 5
  0   -0.749 +1.189 -0.815 +0.714 -0.373 -0.993    O # O #   O
  1   +0.896 +0.914 +1.161 +1.048 +1.070 +0.222    # # # # #
  2   -1.040 -2.182 -0.605 +0.966 -0.579 +0.575    O O O # O #
  3   +0.866 -1.491 +1.786 +1.688 +0.235 -0.616    # O # #   O
  4   +0.407 -1.901 -0.228 -0.824 -0.753 +0.208      O   O O
  5   -0.206 +0.388 -1.709 -0.222 +1.312 +0.347        O   #
  
  
  Case 11, Class X(1), Centred at 2,4, PP: +0.262 X0.738 O0.000 H0.000
  
        0      1      2      3      4      5       0 1 2 3 4 5
  0   -0.989 +0.190 -0.361 +0.014 +0.325 -0.090    O
  1   +0.405 +1.224 -0.663 -0.515 -1.107 +1.036      # O O O #
  2   -1.397 -1.222 +0.846 -0.958 +1.457 -1.996    O O # O # O
  3   -1.115 +1.334 +0.466 +1.615 -1.685 +0.520    O #   # O #
  4   -0.893 +0.527 +1.885 -2.083 +0.379 -0.821    O # # O   O
  5   -0.515 +0.866 +1.333 +3.989 -1.795 +2.070    O # # # O #


  Case 26, Class H(3), Centred at 1,2, PP: +0.685 X0.000 O0.043 H0.272
  
        0      1      2      3      4      5       0 1 2 3 4 5
  0   +1.124 +0.279 -1.360 +1.129 -2.594 -1.462    #   O # O O
  1   +2.117 -0.007 +1.303 +1.715 +0.967 -2.293    #   # # # O
  2   +1.977 +1.098 -1.507 +0.632 -0.636 +0.802    # # O # O #
  3   +0.374 -0.035 +0.831 +0.459 -1.833 -0.452        #   O
  4   -0.773 -0.277 +1.954 +0.913 +0.932 +0.122    O   # # #
  5   +0.195 +1.465 +0.283 -0.521 +0.650 +2.753      #   O # #


The diagrams at the right show the pixels of the image thresholded at
values less than -0.5 (O), greater than +0.5 (#), and between -0.5 and
+0.5 (space).  For the second image, noise in the pixels has made the
symbol less certain (probability 0.738 for the correct symbol) than
for the first case (probability 0.992 for the correct symbol), and for
the third image, the correct symbol has lower probability (0.272) than
one of the incorrect symbols (0.685) - with all these probabilities
computed assuming the true generation model is known.

The igen program computes the performance on test cases when using the
true model, which is an upper limit on what can be accomplished when
learning a model from the training cases (unless one just is lucky).
Here is the summary:

  Error rate on test cases with true model: 0.136
  Average squared error for test cases with true model: 0.196
  Average log probability for test cases with true model: -0.353


A fully-connected Bayesian neural network model.


A convolutional Bayesian neural network model.


Fitting the models with gradient descent.
</pre>
</body>
</html>
