
NOTES ON THE VERSION UNDER DEVELOPMENT

Documentation changes in this version.

1) Detailed documentation on the neural network models is now provided
   in doc/net-models.PDF.  Some references to the models and Markov
   chain methods used are now listed in References.doc.

2) All the tutorial examples have been updated, with timing results now
   given for a modern processor, with some plots now supplied as PNG 
   files, and with other updates and minor fixes.

Feature changes in the version:

1) For neural network models, the configuration of the connections and
   their weights into a hidden layer, from the input layer or the
   previous hidden layer, as well as the hidden unit biases, may now
   be set up explicitly, rather than (as previously) layers always
   being fully-connected, without weight sharing.  In particular, this
   allows for specification of models with convolutional layers.  See
   net-spec.doc and net-config.doc for details, as well as the
   tutorial example in Ex-image.doc.

2) Hidden layers may now use the 'softplus' activation function, given
   by h(u) = log(1+exp(u)), the 'square' activation function, given by
   h(u) = x^2, or the 'cube' activation function, given by h(u) = x^3,
   in addition to the previous options of tanh, sin, or identity.  The
   activaton function may be specified as documented in net-spec.doc.

3) The sample-hyper operation for neural network models can now take
   an argument in order to restrict the updates to the hyperparameters
   controlling a single group of parameters.  This may be useful in
   the initial stages of sampling, to avoid some hyperparameters
   taking on bad values when the data has not yet been fit well.

4) The net-eval program can now optionally display the values of
   hidden units in some layer, instead of the final output.

5) The data-spec command now has an optional -e argument that causes
   the training and test inputs and targets read to be echoed to
   standard output.  This can be useful in checking that the data
   source is specified correctly.

6) New C0, C1, and Cn for n>1 quantities are now defined, to help assess
   how well metropolis and hybrid updates are exploring the distribution.
   (But note that these are masked for mixture models, where they have
   another meaning.)

7) The net-gen program has been extended to allow parameters to be set
   from standard input, rather than being set to zero, or randomly,

8) The xxx-tbl commands can now take a -f argument, causes them to
   continually follow iterations being added to the last log file
   specified, rather than finishing when EOF is encountered.  This is
   useful if the output is piped to a plotting program that is capable
   of following continuing input, updating the plot in real time.

9) The maximum number of hidden layers in a neural network is now 15
   (up from 7 before).

10) The maximum number of iterations that can be used when making
    predictions with the median has been increased from 200 to 1000.

Performance changes in this version:

1) Forward / backward / gradient computations for neural networks have
   been sped up, by rewriting the portable code to encourage the
   compiler to use vector instructions, and by (optionally) using
   specially-written code to exploit SIMD and FMA instructions, when
   available (SSE2, SSE3, SSE4.2, AVX, and AVX2 are supported).

2) Computations for neural networks have also been sped up by using
   the SLEEF library for vectorized mathematical functions (eg, tanh).

3) The precisons for parameter and unit values in neural network
   models may now be either double (FP64), as previously, or float
   (FP32).  Using lower precision will typically speed up the
   computations (but with some effect on the results).

4) Computations for neural network models may now be (partially) done
   on a GPU (one that support CUDA with compute capability 3.5 or
   later).  See Install.doc for how to do this.  Note that for
   networks with few parameters, or that are trained on a small data
   set, the GPU version will not necessarily be faster.

Bug fixes.

1) Fixed a bug in network function evaluation when input offsets are
   present and connections from some inputs are omitted.

2) Documentation for the 'plot' mc operation has been corrected.

3) Incorrect prior specification (not matching ccmds.net) corrected in
   Ex_netgp-c.doc (and Ex_netgp-c.html).

Known bugs and other deficiencies.

1) The facility for plotting quantities using "plot" operations in xxx-mc
   doesn't always work for the first run of xxx-mc (before any
   iterations exist in the log file).  A work-around is to do a run of
   xxx-mc to produce just one iteration before attempting a run of
   xxx-mc that does any "plot" operations.

2) The CPU time features (eg, the "k" quantity) will not work correctly
   if a single iteration takes more than about 71 minutes.

3) The latent value update operations for Gaussian processes may recompute 
   the inverse covariance matrix even when an up-to-date version was 
   computed for the previous Monte Carlo operation.

4) Covariance matrices are stored in full, even though they are symmetric,
   which sometimes costs a factor of two in memory usage.

5) Giving net-pred several log files that have different network architectures
   doesn't work, but an error message is not always produced (the results may
   just be nonsense).

6) Some Markov chain updates for Dirichlet diffusion tree models in which 
   there is no data model (ie, no noise) are not implemented when some of 
   the data is missing.  An error message is produced in such cases.

7) The times given in the documentation for the examples are for a
   very old computer.  Typical current (2020) desktop computers will
   be at least 20 times faster.
