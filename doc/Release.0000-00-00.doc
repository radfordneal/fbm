
NOTES ON THE VERSION UNDER DEVELOPMENT

Changes in this version.

1) Detailed documentation on the neural network models is now provided
   in doc/net-models.PDF.  Some references to the models and Markov
   chain methods used are now listed in References.doc.

2) All the tutorial examples have been updated, with timing results now
   given for a modern processor, with some plots now supplied as PNG 
   files, and with other updates and minor fixes.

3) For neural network models, the configuration of the connections and
   their weights into a hidden layer, from the input layer or the
   previous hidden layer, as well as the hidden unit biases, may now
   be set up explicitly, rather than (as previously) layers always
   being fully-connected, without weight sharing.  In particular, this
   allows for specification of models with convolutional layers.  See
   net-spec.doc and net-config.doc for details, as well as the
   tutorial example in Ex-image.doc.

4) The 'softplus' activation function, h(u) = log(1+exp(u)), has been
   implemented.  It may be specified as documented in net-spec.doc.
   Note that the stepsize heuristics may not work as well for this 
   activation function as for tanh.

5) The sample-hyper operation for neural network models can now take
   an argument in order to restrict the updates to the hyperparameters
   controlling a single group of parameters.  This may be useful in
   the initial stages of sampling, to avoid some hyperparameters
   taking on bad values when the data has not yet been fit well.

6) The net-eval program can now optionally display the values of
   hidden units in some layer, instead of the final output.

7) Forward / backward / gradient computations for neural networks have
   been sped up, by rewriting the portable code to encourage the
   compiler to use vector instructions, and by (optionally) using
   specially-written code to exploit AVX instructions (with or without
   FMA), when available.

8) Computations for neural networks have also been sped up by using
   the SLEEF library for vectorized mathematical functions (eg, tanh).

9) The data-spec command now has an optional -e argument that causes
   the training and test inputs and targets read to be echoed to
   standard output.  This can be useful in checking that the data
   source is specified correctly.

10) New C0, C1, and Cn for n>1 quantities are now defined, to help assess
    how well metropolis and hybrid updates are exploring the distribution.
    (But note that these are masked for mixture models, where they have
    another meaning.)

11) The net-gen program has been extended to allow parameters to be set
    from standard input, rather than being set to zero, or randomly,

12) The xxx-tbl commands can now take a -f argument, causes them to
    continually follow iterations being added to the last log file
    specified, rather than finishing when EOF is encountered.  This is
    useful if the output is piped to a plotting program that is capable
    of following continuing input, updating the plot in real time.

13) The maximum number of hidden layers in a neural network is now 15
    (up from 7 before).

14) The maximum number of iterations that can be used when making
    predictions with the median has been increased from 200 to 1000.

Bug fixes.

1) Fixed a bug in network function evaluation when input offsets are
   present and connections from some inputs are omitted.

2) Documentation for the 'plot' mc operation has been corrected.

3) Incorrect prior specification (not matching ccmds.net) corrected in
   Ex_netgp-c.doc (and Ex_netgp-c.html).

Known bugs and other deficiencies.

1) The facility for plotting quantities using "plot" operations in xxx-mc
   doesn't always work for the first run of xxx-mc (before any
   iterations exist in the log file).  A work-around is to do a run of
   xxx-mc to produce just one iteration before attempting a run of
   xxx-mc that does any "plot" operations.

2) The CPU time features (eg, the "k" quantity) will not work correctly
   if a single iteration takes more than about 71 minutes.

3) The latent value update operations for Gaussian processes may recompute 
   the inverse covariance matrix even when an up-to-date version was 
   computed for the previous Monte Carlo operation.

4) Covariance matrices are stored in full, even though they are symmetric,
   which sometimes costs a factor of two in memory usage.

5) Giving net-pred several log files that have different network architectures
   doesn't work, but an error message is not always produced (the results may
   just be nonsense).

6) Some Markov chain updates for Dirichlet diffusion tree models in which 
   there is no data model (ie, no noise) are not implemented when some of 
   the data is missing.  An error message is produced in such cases.

7) The times given in the documentation for the examples are for a
   very old computer.  Typical current (2020) desktop computers will
   be at least 20 times faster.
