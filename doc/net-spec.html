<html>
<body>
<pre>


NET-SPEC:  Create a new network, or display specifications for existing net.

Net-spec creates a log file containing records describing the network
architecture and the associated priors.  The network architectures
that can be specified are described in <A HREF="net.html">net.doc</A> and <A HREF="net-models.PDF">net-models.PDF</A>.
When invoked with just a log file as argument, net-spec displays the
specifications of the network stored in that log file.

Usage:

    net-spec log-file N-inputs { N-hidden [ act-func ] [ product ] } N-outputs
             / { group=prior [ omit-spec | config-spec ] } 

or: 

    net-spec log-file [ "sizes" ] [ "config" ]

The first form above is for specifying a network archicture. 

The second form is for displaying the network architecture and priors
stored in the specified log file (including the numbering of groups,
useful, for example, in connection with the Wn quantity).  The
optional "config" argument causes the detailed configurations from
"config" specifications to be displayed, in fully-expanded form (as
individual triplets or pairs).  If the optional "sizes" argument is
specified, the numbers of parameters and connections for each group
are output, along with the totals.  (The number of connections will
equal the number of parameters except when the group has a "config"
specification.)  The offset of each group of parameters is also
displayed (useful, for example, when interpreting the output of
net-display -P).


Specifying layer sizes and activation functions.

N-inputs and N-outputs are the numbers of units in the input and
output layers.  The sizes of zero or more hidden layers are specified
between these, with the activation function used to compute the output
of a unit in the layer from its input following the size, along
perhaps with unit(s) of a previous hidden layer that multiply the
value after the activation function is applied.

Possible activation functions are:

    tanh
    softplus
    softplus0
    identity  
    softmax[%channels|/channels]
    normalize[%channels|/channels]

The default is 'tanh', the hyperbolic tangent function.  

The 'softplus' activation function is h(u) = log(1+exp(u)).  The
'softplus0' function is similar, h(u) = log(1+exp(u))-log(2), which
has value zero when the input is zero.

For the 'identity' activation function, the output of a hidden unit is
identical to its summed input.

For the 'softmax' activation function, the output of a unit is the
exponential of its input, divided by the sum of these exponentials for
all units of the layer.  (Hence, the output is between 0 and 1, and
depends on the inputs for all the units, not just the input for that
unit.)  Note that this is the same as the computation of probabilities
for the 'class' model from values of output units.

For the 'normalize' activation function, the output of a unit is its
input divided by the square root of the average squared value of the
inputs for all units in the layer.  (Hence, the output of a hidden
unit depends on the inputs for all the units, not just the input for
that unit.)  To avoid undefined or unstable results when the inputs
are all close to zero, the average squared value of the inputs is
defined as (1/N) (0.01 + SUM x_i^2), where N is the number of units
and x_i^2 is the square of the input for the i'th unit.  The addition
of 0.01 ensures that the average will not be zero.

The softmax and normalize computation may be done separately for
groups of units defined according to the number of "channels", which
must be a divisor of the number of units in the layer.  This can be
done in two ways.  If units are numbered from zero, then for softmax%n
or normalize%n, units i and j are in the same group if i%n equals j%n,
whereas for softmax/n or normalize/n, units i and j are in the same
group if i/n equals j/n (with truncated integer division).

For example, if there are 40 units, 'normalize%4' will normalize the
units separately for 4 channels, each consisting of 10 units.  The
default is one channel - ie, the normalization is done for all units,
as described above.  When there are K channels, the first units for
each channel come first, followed by the second units for each
channel, etc.  So the first group consists of units 0, K, 2K, ...,
the second group consists of units 1, K+1, 2K+1, ..., and so forth.
On the other hand, 'normalize/4' will normalize 10 groups of 4 units,
with each group containing one unit from each channel.  So the first
group consists of units 0, 1, 2, and 4, the second group consists of
units 5, 6, 7, and 8, and so forth.

After application of the activation function, the value of units in a
hidden layer may be multiplied by the value of a unit in an earlier
hidden layer, or each unit may be multiplied by the corresponding unit
of an earlier hidden layer.  This option has the form

    product:&lt;hidden-layer&gt;[.&lt;unit&gt;]

where &lt;hidden-layer&gt; is the number of an earlier hidden layer (from
0), and &lt;unit&gt; is the index of a single unit in that layer (from 1),
which is used to multiply the values of all unit values.  If &lt;unit&gt; is
not specified, the earlier hidden layer must have the same number of
units as the layer it multiplies, with corresponding unit values being
multiplied.

Specifying layer connections and priors.

Following a "/", the priors for groups of weights, biases, offsets,
and adjustments are specified.  For the syntax of a prior
specification (except for adjustments), see <A HREF="prior.html">prior.doc</A>.  If "-" is
given instead of a prior specification (or if a prior specification is
omitted entirely), the parameters in question do not exist at all,
which is equivalent to their being zero.  For adjustments, the prior
is specified by a single "alpha" value.  A value of "-" represents
infinity (effectively eliminating the adjustment).  Omitting a prior
for a set of adjustments also eliminates them.

The prior of a group of input-to-hidden or input-output weights may be
followed by a specification of inputs to be omitted, with the form:

    omit:[-]&lt;input&gt;{,&lt;input&gt;}

This specifies that connections from certain inputs are to be omitted
from those feeding into this layer.  If the "-" is placed before the
list of inputs, the list gives the inputs that are NOT omitted.
Inputs are numbered starting with 1.

The prior for a group of weights for input-hidden, input-output,
hidden-hidden, or hidden-output connections, or for hidden or output
biases may have the configuration of connections for the weights or
biases (with possible sharing) specified with an argument of the form:

    config:&lt;file&gt;{,&lt;file&gt;}

If more than one file is specified, they are concatenated to form the
final configuration file.  If &lt;file&gt; starts with '%', the remainder is
a shell command whose output is used.  If &lt;file&gt; contains a space or
'=', and does not start with '%', it is taken to be a literal string
to be used as the configuration (perhaps concatenated with other
parts).  Note that as a consequence of these rules, configuration file
names must not contain any space, ",", "=" characters, or start with
the "%" character

If a configuration file cannot be opened, it is looked for in the
'configs' subdirectory of the main FBM directory.

This may not be combined with an "omit" specification.  It is also not
allowed if the prior used specifies a value for Alpha-sub-group (see
<A HREF="prior.html">prior.doc</A>), since sub-groups by source unit are not well-defined when
weights may be shared.  The prior must also not have automatic width
scaling (no "x" option).  Finally, the prior for weights in a group
with a configuration file is not affected by the adjustments for its
destination layer

See <A HREF="net-config.html">net-config.doc</A> for documentation on how a configuration is
specified in a file.  The named configuration files do not need to
exist when the configuration is specified; they are read (and checked
for errors) only when needed later.  See also <A HREF="net-config-check.html">net-config-check.doc</A> for
documentation on a command for checking and displaying configurations.

Groups of parameters are identified by name, sometimes with a hidden
layer number (from 0), as follows:

    ti      offsets for the inputs
    ih#     weights from inputs to hidden layer # (default for # is 0)
    bh#     biases for hidden layer # (default for # is 0)
    th#     offsets for hidden layer # (default for # is 0)
    h#h#    weights from the hidden layer identified by the first #
            to the hidden layer identified by the second # (which must
            be greater than the first)
    hh#     weights from hidden layer # minus 1 to hidden layer # 
            (default for # is 1)
    h#o     weights from hidden layer # to outputs (default for # is
            the last hidden layer)
    ho#     same as h#o
    io      weights from inputs to outputs
    bo      biases for outputs
    ah#     adjustments for hidden layer # (default for # is 0)
    ao      adjustments for outputs

The hidden layers are numbered starting with 0.

Note that a data model will also have to be specified (with model-spec)
if any learning is to be done.  

Two records are written to the newly created log file - one with type
'A' with a net_arch structure as data, and one with type 'P' with a
net_priors structure as data.  A record of type 'F' will be written as
well if there are "omit" or "config" specifications.  These records
have an index of -1.  When the second form of the command is used
(with just the log file as argument), these records are read and the
information they contain is displayed on standard output.

            Copyright (c) 1995-2022 by Radford M. Neal
</pre>
</body>
</html>
