#!/bin/bash

# Commands to apply Bayesian neural network to an image classification problem.
# This version uses one hidden layer with convolutional connections and a
# second hidden layer fully connected to the first hidden layer.  Both
# hidden layers connect to the outputs.
#
# Four runs are done, each on three quarters of the training set, with
# the left-out quarters used to assess things like convergence.  Final
# testing is done on the cases not in the training set.

logb=ilog3

for run in 1 2 3 4; do (

  log=$logb.net$run
  end=`calc 400\*$run`
  start=`calc $end-399`

  net-spec $log 36 80 input-config:iconfig 12 4 \
   / ih=0.2:2 bh0=0.3:2 ho0=0.1:3::4 hh=0.4:3::4 bh1=0.2:2 ho1=0.05:4 bo=0.1:2
  model-spec $log class
  
  data-spec $log 36 1 4 / idata-train@-$start:$end . idata-train@$start:$end .

  rand-seed $log $run
  
  net-gen $log fix 0.08

  mc-spec $log repeat 240 heatbath hybrid 100:10 0.03
  net-mc $log 1

  mc-spec $log repeat 120 sample-hyper 1 sample-hyper 2 \
                          sample-hyper 6 sample-hyper 7 \
                          heatbath hybrid 200:20 0.04
  net-mc $log 2

  mc-spec $log repeat 60 sample-hyper 1 sample-hyper 2 \
                         sample-hyper 6 sample-hyper 7 \
                         heatbath hybrid 400:40 0.06
  net-mc $log 3

  mc-spec $log repeat 30 sample-hyper 1 sample-hyper 2 \
                         sample-hyper 6 sample-hyper 7 \
                         heatbath hybrid 800:80 0.15
  net-mc $log 4
  
  mc-spec $log repeat 24 sample-hyper 1 sample-hyper 2 \
                         sample-hyper 6 sample-hyper 7 \
                         heatbath 0.87 hybrid 1000:100 0.2 negate
  net-mc $log 500
  
  mc-spec $log repeat 24 sample-hyper \
                         heatbath 0.93 hybrid 1000:100 0.14 negate
  net-mc $log 3000

) & done

wait
  
./iend.net $logb 751:
